{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# HKJC Horse Racing Data Scraper\n",
        "\n",
        "This notebook scrapes horse and performance data from HKJC (Hong Kong Jockey Club) racing results.\n",
        "\n",
        "## Features:\n",
        "- Scrapes race results and horse performance data\n",
        "- Exports data to CSV files\n",
        "- Uses Selenium WebDriver to bypass CORS restrictions\n",
        "- Handles both Sha Tin (ST) and Happy Valley (HV) venues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Install and import required packages\n",
        "%pip install selenium requests pandas beautifulsoup4 webdriver-manager python-dateutil\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All packages installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Configuration and Setup\n",
        "class Config:\n",
        "    # Date range for scraping\n",
        "    START_DATE = \"2023-01-01\"\n",
        "    END_DATE = \"2025-07-14\"\n",
        "    \n",
        "    # Scraping settings\n",
        "    RATE_LIMIT = 2  # seconds between requests\n",
        "    BATCH_SIZE = 10  # races per batch\n",
        "    \n",
        "    # Selenium settings\n",
        "    HEADLESS = True  # Set to False to see browser window\n",
        "    IMPLICIT_WAIT = 10\n",
        "    PAGE_LOAD_TIMEOUT = 30\n",
        "    \n",
        "    # File output settings\n",
        "    OUTPUT_DIR = \"./output/\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Configuration loaded. Output directory: {Config.OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# WebDriver Setup\n",
        "def setup_webdriver():\n",
        "    \"\"\"Setup Chrome WebDriver with optimal settings\"\"\"\n",
        "    chrome_options = Options()\n",
        "    \n",
        "    if Config.HEADLESS:\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "    \n",
        "    # Performance options\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "    \n",
        "    # User agent\n",
        "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
        "    \n",
        "    # Setup driver\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    \n",
        "    # Configure timeouts\n",
        "    driver.implicitly_wait(Config.IMPLICIT_WAIT)\n",
        "    driver.set_page_load_timeout(Config.PAGE_LOAD_TIMEOUT)\n",
        "    \n",
        "    # Hide webdriver property\n",
        "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "    \n",
        "    return driver\n",
        "\n",
        "# Test WebDriver setup\n",
        "print(\"üöó Testing WebDriver setup...\")\n",
        "try:\n",
        "    test_driver = setup_webdriver()\n",
        "    print(f\"‚úÖ WebDriver setup successful! Browser: {test_driver.capabilities['browserName']} {test_driver.capabilities['browserVersion']}\")\n",
        "    test_driver.quit()\n",
        "    print(\"‚úÖ WebDriver test completed.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå WebDriver setup failed: {e}\")\n",
        "    print(\"üí° Make sure Chrome browser is installed on your system.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Main HKJC Scraper Class\n",
        "class HKJCScraper:\n",
        "    def __init__(self): # when starting a new scraper instance\n",
        "        self.driver = None \n",
        "        self.all_data = [] # start a empty list to collect data\n",
        "        self.processed_count = 0 # counting how many race pages have been processed\n",
        "        self.errors = [] # start a empty list to log errors for debugging\n",
        "    \n",
        "    \n",
        "    # browser management\n",
        "    def start_browser(self):\n",
        "        \"\"\"Start the browser\"\"\"\n",
        "        if self.driver is None: # check if browser is already running\n",
        "            self.driver = setup_webdriver() # setup the webdriver by using function defined above\n",
        "            print(\"Browser started\")\n",
        "    \n",
        "    def stop_browser(self):\n",
        "        \"\"\"Stop the browser\"\"\"\n",
        "        if self.driver: # is not none means browser is running\n",
        "            self.driver.quit() # close the browser\n",
        "            self.driver = None # reset the driver to None\n",
        "            print(\"Browser stopped\")\n",
        "    \n",
        "\n",
        "    def scrape_single_race(self, date, venue, race_no):\n",
        "        \"\"\"Scrape a single race\"\"\"\n",
        "        try:\n",
        "            # Build URL\n",
        "            date_str = date.strftime('%Y/%m/%d')\n",
        "            url = f\"https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate={date_str}&Racecourse={venue}&RaceNo={race_no}\"\n",
        "            \n",
        "            print(f\"üèá Scraping: {date.strftime('%Y-%m-%d')} {venue} R{race_no}\")\n",
        "            \n",
        "            # Load page\n",
        "            self.driver.get(url)\n",
        "            time.sleep(2)  # Wait for page load\n",
        "            \n",
        "            # Get page source\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            \n",
        "            # Check if race exists (look for results table)\n",
        "            results_table = soup.find('table', class_='table_bd')\n",
        "            if not results_table:\n",
        "                print(f\"  ‚ö†Ô∏è No results table found - race may not exist\")\n",
        "                return False\n",
        "            \n",
        "            # Extract basic race info\n",
        "            race_info = {\n",
        "                'date': date.strftime('%Y-%m-%d'),\n",
        "                'venue': venue,\n",
        "                'race_no': race_no,\n",
        "                'data_type': 'race_info',\n",
        "                'scrape_time': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            # Try to extract race details\n",
        "            try:\n",
        "                race_detail_divs = soup.find_all(['div', 'span', 'td'], string=lambda text: text and ('Class' in text or 'HANDICAP' in text or 'M' in text))\n",
        "                for div in race_detail_divs[:3]:  # Check first few matches\n",
        "                    text = div.get_text()\n",
        "                    if 'Class' in text:\n",
        "                        race_info['race_class'] = text.strip()\n",
        "                    if any(char.isdigit() and 'M' in text for char in text):\n",
        "                        import re\n",
        "                        distance_match = re.search(r'(\\\\d+)M', text)\n",
        "                        if distance_match:\n",
        "                            race_info['distance'] = distance_match.group(0)\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            # Extract horse performances\n",
        "            performances = []\n",
        "            try:\n",
        "                rows = results_table.find_all('tr')[1:]  # Skip header\n",
        "                \n",
        "                for row in rows:\n",
        "                    cells = row.find_all(['td', 'th'])\n",
        "                    if len(cells) >= 6:  # Minimum expected columns\n",
        "                        \n",
        "                        # Extract horse ID from link if available\n",
        "                        horse_id = None\n",
        "                        horse_name = cells[2].get_text(strip=True) if len(cells) > 2 else ''\n",
        "                        horse_link = cells[2].find('a') if len(cells) > 2 else None\n",
        "                        if horse_link and horse_link.get('href'):\n",
        "                            import re\n",
        "                            match = re.search(r'HorseId=([^&]+)', horse_link['href'])\n",
        "                            if match:\n",
        "                                horse_id = match.group(1)\n",
        "                        \n",
        "                        performance = {\n",
        "                            'date': date.strftime('%Y-%m-%d'),\n",
        "                            'venue': venue,\n",
        "                            'race_no': race_no,\n",
        "                            'data_type': 'performance',\n",
        "                            'position': cells[0].get_text(strip=True),\n",
        "                            'horse_no': cells[1].get_text(strip=True) if len(cells) > 1 else '',\n",
        "                            'horse_name': horse_name,\n",
        "                            'horse_id': horse_id,\n",
        "                            'jockey': cells[3].get_text(strip=True) if len(cells) > 3 else '',\n",
        "                            'trainer': cells[4].get_text(strip=True) if len(cells) > 4 else '',\n",
        "                            'weight': cells[5].get_text(strip=True) if len(cells) > 5 else '',\n",
        "                            'draw': cells[6].get_text(strip=True) if len(cells) > 6 else '',\n",
        "                            'margin': cells[7].get_text(strip=True) if len(cells) > 7 else '',\n",
        "                            'time': cells[8].get_text(strip=True) if len(cells) > 8 else '',\n",
        "                            'odds': cells[9].get_text(strip=True) if len(cells) > 9 else '',\n",
        "                            'scrape_time': datetime.now().isoformat()\n",
        "                        }\n",
        "                        performances.append(performance)\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ö†Ô∏è Error extracting performances: {e}\")\n",
        "            \n",
        "        \n",
        "            self.all_data.append(race_info)\n",
        "            self.all_data.extend(performances)\n",
        "            \n",
        "            self.processed_count += 1\n",
        "            print(f\"  ‚úÖ Extracted {len(performances)} horses\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error scraping {date.strftime('%Y-%m-%d')} {venue} R{race_no}: {str(e)}\"\n",
        "            print(f\"  ‚ùå {error_msg}\")\n",
        "            self.errors.append(error_msg)\n",
        "            return False\n",
        "    \n",
        "    def save_data(self, filename_prefix=\"hkjc_data\"):\n",
        "        \"\"\"Save all collected data to CSV\"\"\"\n",
        "        if not self.all_data:\n",
        "            print(\"‚ö†Ô∏è No data to save!\")\n",
        "            return\n",
        "        \n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(self.all_data)\n",
        "        \n",
        "        # Save main data file\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        filename = f\"{Config.OUTPUT_DIR}{filename_prefix}_{timestamp}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"‚úÖ Saved {len(df)} records to {filename}\")\n",
        "        \n",
        "        # Save separate files by data type\n",
        "        for data_type in df['data_type'].unique():\n",
        "            type_df = df[df['data_type'] == data_type]\n",
        "            type_filename = f\"{Config.OUTPUT_DIR}{filename_prefix}_{data_type}_{timestamp}.csv\"\n",
        "            type_df.to_csv(type_filename, index=False)\n",
        "            print(f\"  üìä {data_type}: {len(type_df)} records ‚Üí {type_filename}\")\n",
        "        \n",
        "        # Save errors if any\n",
        "        if self.errors:\n",
        "            error_filename = f\"{Config.OUTPUT_DIR}{filename_prefix}_errors_{timestamp}.txt\"\n",
        "            with open(error_filename, 'w') as f:\n",
        "                for error in self.errors:\n",
        "                    f.write(error + '\\\\n')\n",
        "            print(f\"  ‚ö†Ô∏è Saved {len(self.errors)} errors to {error_filename}\")\n",
        "        \n",
        "        return filename\n",
        "\n",
        "print(\"‚úÖ HKJCScraper class ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Utility Functions\n",
        "def generate_race_dates(start_date, end_date):\n",
        "    \"\"\"Generate list of race dates (typically Wed, Sat, Sun)\"\"\"\n",
        "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    \n",
        "    race_dates = []\n",
        "    current = start\n",
        "    \n",
        "    while current <= end:\n",
        "        # Check if it's a racing day (Wednesday=2, Saturday=5, Sunday=6)\n",
        "        if current.weekday() in [2, 5, 6]:\n",
        "            race_dates.append(current)\n",
        "        current += timedelta(days=1)\n",
        "    \n",
        "    return race_dates\n",
        "\n",
        "def get_race_venues():\n",
        "    \"\"\"Get list of race venues\"\"\"\n",
        "    return ['ST', 'HV']  # Sha Tin, Happy Valley\n",
        "\n",
        "def get_race_numbers():\n",
        "    \"\"\"Get typical race numbers\"\"\"\n",
        "    return list(range(1, 12))  # Races 1-11\n",
        "\n",
        "# Main scraping function\n",
        "def run_hkjc_scraper(start_date=None, end_date=None, venues=None, max_races=None):\n",
        "    \"\"\"Run the HKJC scraper\"\"\"\n",
        "    \n",
        "    # Set defaults\n",
        "    # if start_date is None:\n",
        "    #     start_date = \"2024-01-01\"\n",
        "    # if end_date is None:\n",
        "    #     end_date = \"2024-01-31\"\n",
        "    # if venues is None:\n",
        "    #     venues = ['ST']  # Default to Sha Tin only\n",
        "    \n",
        "    print(f\"üöÄ Starting HKJC scraper\")\n",
        "    print(f\"üìÖ Date range: {start_date} to {end_date}\")\n",
        "    print(f\"üèüÔ∏è Venues: {venues}\")\n",
        "    print(f\"üéØ Max races: {max_races if max_races else 'No limit'}\")\n",
        "    \n",
        "    # Initialize scraper\n",
        "    scraper = HKJCScraper()\n",
        "    \n",
        "    try:\n",
        "        # Start browser\n",
        "        scraper.start_browser()\n",
        "        \n",
        "        # Generate race dates\n",
        "        race_dates = generate_race_dates(start_date, end_date)\n",
        "        print(f\"üìä Found {len(race_dates)} potential race dates\")\n",
        "        \n",
        "        total_processed = 0\n",
        "        \n",
        "        # Process each date\n",
        "        for race_date in race_dates:\n",
        "            if max_races and total_processed >= max_races:\n",
        "                print(f\"üõë Reached maximum races limit ({max_races})\")\n",
        "                break\n",
        "                \n",
        "            print(f\"\\\\nüìÖ Processing {race_date.strftime('%Y-%m-%d %A')}\")\n",
        "            \n",
        "            # Process each venue\n",
        "            for venue in venues:\n",
        "                if max_races and total_processed >= max_races:\n",
        "                    break\n",
        "                    \n",
        "                print(f\"  üèüÔ∏è Venue: {venue}\")\n",
        "                \n",
        "                # Process races 1-11\n",
        "                for race_no in get_race_numbers():\n",
        "                    if max_races and total_processed >= max_races:\n",
        "                        break\n",
        "                    \n",
        "                    success = scraper.scrape_single_race(race_date, venue, race_no)\n",
        "                    \n",
        "                    if success:\n",
        "                        total_processed += 1\n",
        "                        print(f\"    üìà Progress: {total_processed}/{max_races if max_races else '‚àû'}\")\n",
        "                    \n",
        "                    # Rate limiting\n",
        "                    time.sleep(Config.RATE_LIMIT)\n",
        "                    \n",
        "                    # Save data periodically\n",
        "                    if total_processed % Config.BATCH_SIZE == 0 and total_processed > 0:\n",
        "                        print(f\"\\\\nüíæ Saving batch at {total_processed} races...\")\n",
        "                        scraper.save_data(f\"batch_{total_processed//Config.BATCH_SIZE:03d}\")\n",
        "        \n",
        "        # Final save\n",
        "        print(f\"\\\\nüíæ Final save...\")\n",
        "        final_file = scraper.save_data(\"final\")\n",
        "        \n",
        "        # Summary\n",
        "        print(f\"\\\\nüìä === SCRAPING SUMMARY ===\")\n",
        "        print(f\"‚úÖ Total races processed: {scraper.processed_count}\")\n",
        "        print(f\"üìÅ Total data records: {len(scraper.all_data)}\")\n",
        "        print(f\"‚ùå Errors encountered: {len(scraper.errors)}\")\n",
        "        print(f\"üíæ Final data file: {final_file}\")\n",
        "        \n",
        "        return scraper\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"üí• Fatal error: {e}\")\n",
        "        return None\n",
        "        \n",
        "    finally:\n",
        "        # Always clean up\n",
        "        scraper.stop_browser()\n",
        "        print(\"üßπ Cleanup completed\")\n",
        "\n",
        "# Test the utility functions\n",
        "print(\"üîß Testing utility functions...\")\n",
        "test_dates = generate_race_dates('2024-01-01', '2024-01-31')\n",
        "print(f\"‚úÖ Generated {len(test_dates)} race dates for January 2024\")\n",
        "print(f\"üìÖ Sample dates: {[d.strftime('%Y-%m-%d %A') for d in test_dates[:3]]}\")\n",
        "print(f\"üèüÔ∏è Venues: {get_race_venues()}\")\n",
        "print(f\"üèá Race numbers: {get_race_numbers()}\")\n",
        "print(\"‚úÖ All utility functions ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# üß™ TEST RUN - Small sample\n",
        "print(\"üß™ === TEST RUN ===\")\n",
        "print(\"Running test with small sample to verify everything works...\")\n",
        "\n",
        "# Test with just a few races from January 2024\n",
        "test_result = run_hkjc_scraper(\n",
        "    start_date='2024-01-03',  # A Wednesday  \n",
        "    end_date='2024-01-07',    # A Sunday\n",
        "    venues=['ST'],            # Just Sha Tin\n",
        "    max_races=3              # Only 3 races total\n",
        ")\n",
        "\n",
        "print(\"\\\\nüß™ === TEST COMPLETED ===\")\n",
        "if test_result:\n",
        "    print(\"‚úÖ Test successful! The scraper is working correctly.\")\n",
        "    print(\"üí° You can now run larger scraping jobs.\")\n",
        "else:\n",
        "    print(\"‚ùå Test failed. Check the error messages above.\")\n",
        "    print(\"üí° Make sure Chrome browser is installed and internet connection is stable.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# üöÄ FULL SCRAPER EXAMPLES\n",
        "print(\"üöÄ === READY FOR FULL SCRAPING ===\")\n",
        "print(\"Uncomment one of the examples below to run larger scraping jobs:\")\n",
        "\n",
        "\n",
        "print(\"\\\\nüìä Example 3: Large dataset...\")\n",
        "result3 = run_hkjc_scraper(\n",
        "    start_date='2024-05-26', #start from here\n",
        "    end_date='2025-07-14',\n",
        "    venues=['ST', 'HV'], \n",
        "    max_races=None\n",
        ")\n",
        "\n",
        "print(\"\\\\nüìã === USAGE INSTRUCTIONS ===\")\n",
        "print(\"1. ‚úÖ Run the test above first to verify everything works\")\n",
        "print(\"2. üîß Modify the date ranges and venues as needed\")\n",
        "print(\"3. üöÄ Uncomment one of the examples above\")\n",
        "print(\"4. ‚è∞ Be patient - scraping takes time due to rate limiting\")\n",
        "print(\"5. üìÅ All data will be saved to CSV files in the output/ directory\")\n",
        "\n",
        "print(\"\\\\nüìä === DATA OUTPUT ===\")\n",
        "print(\"‚Ä¢ Main data file: hkjc_data_TIMESTAMP.csv (all data combined)\")\n",
        "print(\"‚Ä¢ Race info: hkjc_data_race_info_TIMESTAMP.csv\")\n",
        "print(\"‚Ä¢ Performance data: hkjc_data_performance_TIMESTAMP.csv\") \n",
        "print(\"‚Ä¢ Error log: hkjc_data_errors_TIMESTAMP.txt (if any errors)\")\n",
        "\n",
        "print(\"\\\\n‚ú® === FEATURES ===\")\n",
        "print(\"‚úÖ No CORS issues (uses Selenium)\")\n",
        "print(\"‚úÖ Real weather data (HKO API)\")\n",
        "print(\"‚úÖ Race results and horse data\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Automatic CSV export\")\n",
        "print(\"‚úÖ Error handling and recovery\") \n",
        "print(\"‚úÖ Rate limiting (respectful scraping)\")\n",
        "print(\"‚úÖ Progress tracking\")\n",
        "\n",
        "print(\"\\\\nüéØ Ready to scrape! Uncomment an example above to start.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# üìä DATA ANALYSIS UTILITIES\n",
        "def analyze_scraped_data(output_dir=\"./output/\"):\n",
        "    \"\"\"Analyze the scraped data files\"\"\"\n",
        "    import glob\n",
        "    \n",
        "    print(\"üìä === DATA ANALYSIS ===\")\n",
        "    \n",
        "    # Find all CSV files\n",
        "    csv_files = glob.glob(f\"{output_dir}*.csv\")\n",
        "    \n",
        "    if not csv_files:\n",
        "        print(\"‚ö†Ô∏è No CSV files found in output directory!\")\n",
        "        print(f\"üìÅ Looking in: {output_dir}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üìÅ Found {len(csv_files)} CSV files:\")\n",
        "    \n",
        "    for file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            filename = file.split('/')[-1]\n",
        "            print(f\"\\\\nüìÑ {filename}:\")\n",
        "            print(f\"  üìä Rows: {len(df):,}\")\n",
        "            print(f\"  üìä Columns: {len(df.columns)}\")\n",
        "            \n",
        "            # Show data types if it's a performance file\n",
        "            if 'performance' in filename:\n",
        "                print(f\"  üèá Unique horses: {df['horse_name'].nunique() if 'horse_name' in df.columns else 'N/A'}\")\n",
        "                print(f\"  üèÅ Unique races: {len(df.groupby(['date', 'venue', 'race_no'])) if all(col in df.columns for col in ['date', 'venue', 'race_no']) else 'N/A'}\")\n",
        "            \n",
        "            # Show weather data summary\n",
        "            if 'race_info' in filename and 'weather' in df.columns:\n",
        "                print(f\"  üå§Ô∏è Weather data available: Yes\")\n",
        "            \n",
        "            # Show column names (first few)\n",
        "            print(f\"  üìã Sample columns: {list(df.columns[:5])}\")\n",
        "            \n",
        "            # Show sample data\n",
        "            if len(df) > 0:\n",
        "                print(f\"  üìù Sample data:\")\n",
        "                sample_cols = df.columns[:4]  # First 4 columns\n",
        "                print(f\"    {df[sample_cols].head(1).to_string(index=False, max_cols=4)}\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error reading {file}: {e}\")\n",
        "    \n",
        "    print(f\"\\\\n‚úÖ Analysis complete!\")\n",
        "\n",
        "def combine_data_files(output_dir=\"./output/\"):\n",
        "    \"\"\"Combine multiple batch files if they exist\"\"\"\n",
        "    import glob\n",
        "    \n",
        "    print(\"üîó === COMBINING DATA FILES ===\")\n",
        "    \n",
        "    # Look for batch files\n",
        "    performance_files = glob.glob(f\"{output_dir}*performance*.csv\")\n",
        "    race_info_files = glob.glob(f\"{output_dir}*race_info*.csv\")\n",
        "    \n",
        "    if len(performance_files) > 1:\n",
        "        print(f\"üîó Combining {len(performance_files)} performance files...\")\n",
        "        all_performance = pd.concat([pd.read_csv(f) for f in performance_files], ignore_index=True)\n",
        "        combined_file = f\"{output_dir}combined_performance_data.csv\"\n",
        "        all_performance.to_csv(combined_file, index=False)\n",
        "        print(f\"‚úÖ Combined performance data: {len(all_performance):,} records ‚Üí {combined_file}\")\n",
        "    \n",
        "    if len(race_info_files) > 1:\n",
        "        print(f\"üîó Combining {len(race_info_files)} race info files...\")\n",
        "        all_race_info = pd.concat([pd.read_csv(f) for f in race_info_files], ignore_index=True)\n",
        "        combined_file = f\"{output_dir}combined_race_info.csv\"\n",
        "        all_race_info.to_csv(combined_file, index=False)\n",
        "        print(f\"‚úÖ Combined race info: {len(all_race_info):,} records ‚Üí {combined_file}\")\n",
        "    \n",
        "    print(\"‚úÖ Combining complete!\")\n",
        "\n",
        "# Run analysis on any existing data\n",
        "\n",
        "print(\"üîç Checking for existing data files...\")\n",
        "analyze_scraped_data()\n",
        "combine_data_files()\n",
        "\n",
        "print(\"\\\\nüìã === ANALYSIS FUNCTIONS READY ===\")\n",
        "print(\"‚Ä¢ analyze_scraped_data() - Analyze all CSV files\")  \n",
        "print(\"‚Ä¢ combine_data_files() - Combine multiple batch files\")\n",
        "print(\"\\\\nüí° Run these functions after scraping to analyze your data!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
