{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b3f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import camelot\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa54a2",
   "metadata": {},
   "source": [
    "# Download pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b812f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'fixture_23.pdf' : 'https://res.hkjc.com/racingnews/wp-content/uploads/sites/3/2023/07/fixture_23-24_en.pdf',\n",
    "    'fixture_24.pdf' : 'https://res.hkjc.com/racingnews/wp-content/uploads/sites/3/2024/07/fixture_24-25_e.pdf',\n",
    "    'fixture_25.pdf' : 'https://res.hkjc.com/racingnews/wp-content/uploads/sites/3/2025/07/fixture_25-26.pdf'\n",
    "}\n",
    "\n",
    "out_dir = '../data/fixtures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f56fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(urls, out_dir):\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "    for filename, url in urls.items():\n",
    "        print(f'downloading {filename} from {url}')\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, stream = True, timeout=15)\n",
    "            if response.status_code == 200: \n",
    "                out_path = os.path.join(out_dir + filename)\n",
    "                with open(out_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f'file downloaded to {out_path}')\n",
    "        \n",
    "            else:\n",
    "                print(f'file download failed with url: {url}')\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f'error downloading {url}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f5258f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading fixture_23.pdf from https://res.hkjc.com/racingnews/wp-content/uploads/sites/3/2023/07/fixture_23-24_en.pdf\n",
      "file downloaded to ../data/fixtures/fixture_23.pdf\n",
      "downloading fixture_24.pdf from https://res.hkjc.com/racingnews/wp-content/uploads/sites/3/2024/07/fixture_24-25_e.pdf\n",
      "file downloaded to ../data/fixtures/fixture_24.pdf\n",
      "downloading fixture_25.pdf from https://res.hkjc.com/racingnews/wp-content/uploads/sites/3/2025/07/fixture_25-26.pdf\n",
      "file downloaded to ../data/fixtures/fixture_25.pdf\n"
     ]
    }
   ],
   "source": [
    "download_pdf(urls, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10407c0",
   "metadata": {},
   "source": [
    "# Function to clean fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4444730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the date format\n",
    "def format_date(date_str, year):\n",
    "    try:\n",
    "        # remove line break in 'Date'\n",
    "        date_str = date_str.replace('\\n', ' ')\n",
    "        \n",
    "        # seperate into different parts for extraction\n",
    "        parts = date_str.split()\n",
    "\n",
    "        if len(parts) >= 3:\n",
    "            day = parts[1]\n",
    "            month = parts[2]\n",
    "        else: \n",
    "            return date_str\n",
    "        \n",
    "        dt = datetime.strptime(f\"{day} {month} {year}\", \"%d %b %Y\")\n",
    "        return dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    except Exception:\n",
    "        return date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fac74bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fixture(fixture):\n",
    "    \n",
    "    full_frame = pd.DataFrame()\n",
    "\n",
    "    year_pattern = f'(2022|2023|2024|2025|2026)'\n",
    "\n",
    "    for f in fixture:\n",
    "        # read each pdf\n",
    "        table = camelot.read_pdf(f, pages = 'all', flavor = 'stream')\n",
    "\n",
    "        # extract table 0 for fixture_23, table 1 for others\n",
    "        if 'fixture_23' in f:\n",
    "            fixture = table[0].df\n",
    "        else:\n",
    "            fixture = table[1].df\n",
    "\n",
    "        left = fixture[[0, 1]].rename(columns = {0: 'Date', 1: 'Venue'})\n",
    "        right = fixture[[2, 3]].rename(columns = {2: 'Date', 3: 'Venue'})\n",
    "\n",
    "        # concat two parts into one list\n",
    "        combined = pd.concat([left, right], ignore_index = True)\n",
    "        conbined = combined.dropna(how = 'all')\n",
    "\n",
    "        combined['year'] = None\n",
    "        current_year = None\n",
    "\n",
    "        for idx, val in combined['Date'].items():\n",
    "            if pd.notna(val) and str(val).strip().isdigit() and re.match(year_pattern, str(val).strip()):\n",
    "                current_year = str(val).strip()\n",
    "                combined.at[idx, 'Date'] = None\n",
    "            else:\n",
    "                combined.at[idx, 'year'] = current_year\n",
    "\n",
    "        combined = combined.dropna(subset = ['Date']).reset_index(drop = True)\n",
    "\n",
    "        combined['Date'] = combined.apply(lambda row: format_date(row['Date'], row['year']), axis = 1)\n",
    "        combined = combined.drop(columns = ['year'])\n",
    "\n",
    "        combined = combined.replace(r'^\\s*$', np.nan, regex=True)      \n",
    "        combined = combined.dropna(subset=['Date', 'Venue']).reset_index(drop=True)  \n",
    "\n",
    "        combined = combined[~combined['Date'].str.contains('DATE', case = False, na = False)].reset_index(drop=True)\n",
    "\n",
    "        # Normalise venue column\n",
    "        combined['Venue'] = combined['Venue'].str.strip().replace(\n",
    "            {\n",
    "                r'(?i)^.*sha\\s*tin.*$': 'ST',\n",
    "                r'(?i)^.*happy\\s*valley.*$': 'HV'},\n",
    "            regex=True\n",
    "        )\n",
    "\n",
    "        full_frame = pd.concat([full_frame,combined], ignore_index = True)\n",
    "\n",
    "    return full_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df159ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_list = [\n",
    "    \"../data/fixtures/fixture_23.pdf\",\n",
    "    \"../data/fixtures/fixture_24.pdf\",\n",
    "    \"../data/fixtures/fixture_25.pdf\"\n",
    "]\n",
    "\n",
    "df = clean_fixture(pdf_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c304e",
   "metadata": {},
   "source": [
    "# scrape by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51d02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = ['2025/07/16']\n",
    "venue = ['HV', 'ST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd5faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dates': date,\n",
    "    'venues': venue\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fa2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_race_data_by_venue(date, venues, max_race_no=9):\n",
    " \n",
    "    base_url = \"https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx\"\n",
    "    results = {}\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; HKJCScraper/1.0)\"\n",
    "    }\n",
    "\n",
    "    for venue in venues:\n",
    "        print(f\"Checking venue {venue} race no 1...\")\n",
    "        params = {\n",
    "            \"RaceDate\": date,\n",
    "            \"Racecourse\": venue,\n",
    "            \"RaceNo\": 1\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch page for venue {venue} race 1, status: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Check for the presence of the race result table, often with class 'table_bd' or id\n",
    "        table = soup.find('table', class_='table_bd')\n",
    "\n",
    "        if not table:\n",
    "            print(f\"No race data found for venue {venue} race 1; skipping rest of races for this venue.\")\n",
    "            continue  # Skip race 2-9 for this venue\n",
    "\n",
    "        # Save race 1 result\n",
    "        results[(venue, 1)] = soup\n",
    "        print(f\"Race data found for venue {venue} race 1; fetching races 2 to {max_race_no}...\")\n",
    "\n",
    "        # Now fetch the rest of the races for this venue\n",
    "        for race_no in range(2, max_race_no + 1):\n",
    "            params[\"RaceNo\"] = race_no\n",
    "            response = requests.get(base_url, params=params, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch page for venue {venue} race {race_no}, status: {response.status_code}\")\n",
    "                results[(venue, race_no)] = None\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            table = soup.find('table', class_='table_bd')\n",
    "            if not table:\n",
    "                print(f\"No race data found for venue {venue} race {race_no}.\")\n",
    "                results[(venue, race_no)] = None\n",
    "            else:\n",
    "                results[(venue, race_no)] = soup\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example of calling the function\n",
    "if __name__ == \"__main__\":\n",
    "    race_date = '2025/07/16'\n",
    "    venue_list = ['HV', 'ST']\n",
    "\n",
    "    data = fetch_race_data_by_venue(race_date, venue_list)\n",
    "    # 'data' contains BeautifulSoup objects for successful pages or None entries for misses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
