{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# HKJC Horse Racing Data Scraper\n",
        "\n",
        "This notebook scrapes horse and performance data from HKJC (Hong Kong Jockey Club) racing results.\n",
        "\n",
        "## Features:\n",
        "- Scrapes race results and horse performance data\n",
        "- Exports data to CSV files\n",
        "- Uses Selenium WebDriver to bypass CORS restrictions\n",
        "- Handles both Sha Tin (ST) and Happy Valley (HV) venues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Install and import required packages\n",
        "%pip install selenium requests pandas beautifulsoup4 webdriver-manager python-dateutil\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ All packages installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Configuration and Setup\n",
        "class Config:\n",
        "    # Date range for scraping\n",
        "    START_DATE = \"2023-01-01\"\n",
        "    END_DATE = \"2025-07-14\"\n",
        "    \n",
        "    # Scraping settings\n",
        "    RATE_LIMIT = 2  # seconds between requests\n",
        "    BATCH_SIZE = 10  # races per batch\n",
        "    \n",
        "    # Selenium settings\n",
        "    HEADLESS = True  # Set to False to see browser window\n",
        "    IMPLICIT_WAIT = 10\n",
        "    PAGE_LOAD_TIMEOUT = 30\n",
        "    \n",
        "    # File output settings\n",
        "    OUTPUT_DIR = \"./output/\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"✅ Configuration loaded. Output directory: {Config.OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# WebDriver Setup\n",
        "def setup_webdriver():\n",
        "    \"\"\"Setup Chrome WebDriver with optimal settings\"\"\"\n",
        "    chrome_options = Options()\n",
        "    \n",
        "    if Config.HEADLESS:\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "    \n",
        "    # Performance options\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "    \n",
        "    # User agent\n",
        "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
        "    \n",
        "    # Setup driver\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    \n",
        "    # Configure timeouts\n",
        "    driver.implicitly_wait(Config.IMPLICIT_WAIT)\n",
        "    driver.set_page_load_timeout(Config.PAGE_LOAD_TIMEOUT)\n",
        "    \n",
        "    # Hide webdriver property\n",
        "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "    \n",
        "    return driver\n",
        "\n",
        "# Test WebDriver setup\n",
        "print(\"🚗 Testing WebDriver setup...\")\n",
        "try:\n",
        "    test_driver = setup_webdriver()\n",
        "    print(f\"✅ WebDriver setup successful! Browser: {test_driver.capabilities['browserName']} {test_driver.capabilities['browserVersion']}\")\n",
        "    test_driver.quit()\n",
        "    print(\"✅ WebDriver test completed.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ WebDriver setup failed: {e}\")\n",
        "    print(\"💡 Make sure Chrome browser is installed on your system.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Main HKJC Scraper Class\n",
        "class HKJCScraper:\n",
        "    def __init__(self): # when starting a new scraper instance\n",
        "        self.driver = None \n",
        "        self.all_data = [] # start a empty list to collect data\n",
        "        self.processed_count = 0 # counting how many race pages have been processed\n",
        "        self.errors = [] # start a empty list to log errors for debugging\n",
        "    \n",
        "    \n",
        "    # browser management\n",
        "    def start_browser(self):\n",
        "        \"\"\"Start the browser\"\"\"\n",
        "        if self.driver is None: # check if browser is already running\n",
        "            self.driver = setup_webdriver() # setup the webdriver by using function defined above\n",
        "            print(\"Browser started\")\n",
        "    \n",
        "    def stop_browser(self):\n",
        "        \"\"\"Stop the browser\"\"\"\n",
        "        if self.driver: # is not none means browser is running\n",
        "            self.driver.quit() # close the browser\n",
        "            self.driver = None # reset the driver to None\n",
        "            print(\"Browser stopped\")\n",
        "    \n",
        "\n",
        "    def scrape_single_race(self, date, venue, race_no):\n",
        "        \"\"\"Scrape a single race\"\"\"\n",
        "        try:\n",
        "            # Build URL\n",
        "            date_str = date.strftime('%Y/%m/%d')\n",
        "            url = f\"https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate={date_str}&Racecourse={venue}&RaceNo={race_no}\"\n",
        "            \n",
        "            print(f\"🏇 Scraping: {date.strftime('%Y-%m-%d')} {venue} R{race_no}\")\n",
        "            \n",
        "            # Load page\n",
        "            self.driver.get(url)\n",
        "            time.sleep(2)  # Wait for page load\n",
        "            \n",
        "            # Get page source\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            \n",
        "            # Check if race exists (look for results table)\n",
        "            results_table = soup.find('table', class_='table_bd')\n",
        "            if not results_table:\n",
        "                print(f\"  ⚠️ No results table found - race may not exist\")\n",
        "                return False\n",
        "            \n",
        "            # Extract basic race info\n",
        "            race_info = {\n",
        "                'date': date.strftime('%Y-%m-%d'),\n",
        "                'venue': venue,\n",
        "                'race_no': race_no,\n",
        "                'data_type': 'race_info',\n",
        "                'scrape_time': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            # Try to extract race details\n",
        "            try:\n",
        "                race_detail_divs = soup.find_all(['div', 'span', 'td'], string=lambda text: text and ('Class' in text or 'HANDICAP' in text or 'M' in text))\n",
        "                for div in race_detail_divs[:3]:  # Check first few matches\n",
        "                    text = div.get_text()\n",
        "                    if 'Class' in text:\n",
        "                        race_info['race_class'] = text.strip()\n",
        "                    if any(char.isdigit() and 'M' in text for char in text):\n",
        "                        import re\n",
        "                        distance_match = re.search(r'(\\\\d+)M', text)\n",
        "                        if distance_match:\n",
        "                            race_info['distance'] = distance_match.group(0)\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            # Extract horse performances\n",
        "            performances = []\n",
        "            try:\n",
        "                rows = results_table.find_all('tr')[1:]  # Skip header\n",
        "                \n",
        "                for row in rows:\n",
        "                    cells = row.find_all(['td', 'th'])\n",
        "                    if len(cells) >= 6:  # Minimum expected columns\n",
        "                        \n",
        "                        # Extract horse ID from link if available\n",
        "                        horse_id = None\n",
        "                        horse_name = cells[2].get_text(strip=True) if len(cells) > 2 else ''\n",
        "                        horse_link = cells[2].find('a') if len(cells) > 2 else None\n",
        "                        if horse_link and horse_link.get('href'):\n",
        "                            import re\n",
        "                            match = re.search(r'HorseId=([^&]+)', horse_link['href'])\n",
        "                            if match:\n",
        "                                horse_id = match.group(1)\n",
        "                        \n",
        "                        performance = {\n",
        "                            'date': date.strftime('%Y-%m-%d'),\n",
        "                            'venue': venue,\n",
        "                            'race_no': race_no,\n",
        "                            'data_type': 'performance',\n",
        "                            'position': cells[0].get_text(strip=True),\n",
        "                            'horse_no': cells[1].get_text(strip=True) if len(cells) > 1 else '',\n",
        "                            'horse_name': horse_name,\n",
        "                            'horse_id': horse_id,\n",
        "                            'jockey': cells[3].get_text(strip=True) if len(cells) > 3 else '',\n",
        "                            'trainer': cells[4].get_text(strip=True) if len(cells) > 4 else '',\n",
        "                            'weight': cells[5].get_text(strip=True) if len(cells) > 5 else '',\n",
        "                            'draw': cells[6].get_text(strip=True) if len(cells) > 6 else '',\n",
        "                            'margin': cells[7].get_text(strip=True) if len(cells) > 7 else '',\n",
        "                            'time': cells[8].get_text(strip=True) if len(cells) > 8 else '',\n",
        "                            'odds': cells[9].get_text(strip=True) if len(cells) > 9 else '',\n",
        "                            'scrape_time': datetime.now().isoformat()\n",
        "                        }\n",
        "                        performances.append(performance)\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️ Error extracting performances: {e}\")\n",
        "            \n",
        "        \n",
        "            self.all_data.append(race_info)\n",
        "            self.all_data.extend(performances)\n",
        "            \n",
        "            self.processed_count += 1\n",
        "            print(f\"  ✅ Extracted {len(performances)} horses\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error scraping {date.strftime('%Y-%m-%d')} {venue} R{race_no}: {str(e)}\"\n",
        "            print(f\"  ❌ {error_msg}\")\n",
        "            self.errors.append(error_msg)\n",
        "            return False\n",
        "    \n",
        "    def save_data(self, filename_prefix=\"hkjc_data\"):\n",
        "        \"\"\"Save all collected data to CSV\"\"\"\n",
        "        if not self.all_data:\n",
        "            print(\"⚠️ No data to save!\")\n",
        "            return\n",
        "        \n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(self.all_data)\n",
        "        \n",
        "        # Save main data file\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        filename = f\"{Config.OUTPUT_DIR}{filename_prefix}_{timestamp}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"✅ Saved {len(df)} records to {filename}\")\n",
        "        \n",
        "        # Save separate files by data type\n",
        "        for data_type in df['data_type'].unique():\n",
        "            type_df = df[df['data_type'] == data_type]\n",
        "            type_filename = f\"{Config.OUTPUT_DIR}{filename_prefix}_{data_type}_{timestamp}.csv\"\n",
        "            type_df.to_csv(type_filename, index=False)\n",
        "            print(f\"  📊 {data_type}: {len(type_df)} records → {type_filename}\")\n",
        "        \n",
        "        # Save errors if any\n",
        "        if self.errors:\n",
        "            error_filename = f\"{Config.OUTPUT_DIR}{filename_prefix}_errors_{timestamp}.txt\"\n",
        "            with open(error_filename, 'w') as f:\n",
        "                for error in self.errors:\n",
        "                    f.write(error + '\\\\n')\n",
        "            print(f\"  ⚠️ Saved {len(self.errors)} errors to {error_filename}\")\n",
        "        \n",
        "        return filename\n",
        "\n",
        "print(\"✅ HKJCScraper class ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Utility Functions\n",
        "def generate_race_dates(start_date, end_date):\n",
        "    \"\"\"Generate list of race dates (typically Wed, Sat, Sun)\"\"\"\n",
        "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    \n",
        "    race_dates = []\n",
        "    current = start\n",
        "    \n",
        "    while current <= end:\n",
        "        # Check if it's a racing day (Wednesday=2, Saturday=5, Sunday=6)\n",
        "        if current.weekday() in [2, 5, 6]:\n",
        "            race_dates.append(current)\n",
        "        current += timedelta(days=1)\n",
        "    \n",
        "    return race_dates\n",
        "\n",
        "def get_race_venues():\n",
        "    \"\"\"Get list of race venues\"\"\"\n",
        "    return ['ST', 'HV']  # Sha Tin, Happy Valley\n",
        "\n",
        "def get_race_numbers():\n",
        "    \"\"\"Get typical race numbers\"\"\"\n",
        "    return list(range(1, 12))  # Races 1-11\n",
        "\n",
        "# Main scraping function\n",
        "def run_hkjc_scraper(start_date=None, end_date=None, venues=None, max_races=None):\n",
        "    \"\"\"Run the HKJC scraper\"\"\"\n",
        "    \n",
        "    # Set defaults\n",
        "    # if start_date is None:\n",
        "    #     start_date = \"2024-01-01\"\n",
        "    # if end_date is None:\n",
        "    #     end_date = \"2024-01-31\"\n",
        "    # if venues is None:\n",
        "    #     venues = ['ST']  # Default to Sha Tin only\n",
        "    \n",
        "    print(f\"🚀 Starting HKJC scraper\")\n",
        "    print(f\"📅 Date range: {start_date} to {end_date}\")\n",
        "    print(f\"🏟️ Venues: {venues}\")\n",
        "    print(f\"🎯 Max races: {max_races if max_races else 'No limit'}\")\n",
        "    \n",
        "    # Initialize scraper\n",
        "    scraper = HKJCScraper()\n",
        "    \n",
        "    try:\n",
        "        # Start browser\n",
        "        scraper.start_browser()\n",
        "        \n",
        "        # Generate race dates\n",
        "        race_dates = generate_race_dates(start_date, end_date)\n",
        "        print(f\"📊 Found {len(race_dates)} potential race dates\")\n",
        "        \n",
        "        total_processed = 0\n",
        "        \n",
        "        # Process each date\n",
        "        for race_date in race_dates:\n",
        "            if max_races and total_processed >= max_races:\n",
        "                print(f\"🛑 Reached maximum races limit ({max_races})\")\n",
        "                break\n",
        "                \n",
        "            print(f\"\\\\n📅 Processing {race_date.strftime('%Y-%m-%d %A')}\")\n",
        "            \n",
        "            # Process each venue\n",
        "            for venue in venues:\n",
        "                if max_races and total_processed >= max_races:\n",
        "                    break\n",
        "                    \n",
        "                print(f\"  🏟️ Venue: {venue}\")\n",
        "                \n",
        "                # Process races 1-11\n",
        "                for race_no in get_race_numbers():\n",
        "                    if max_races and total_processed >= max_races:\n",
        "                        break\n",
        "                    \n",
        "                    success = scraper.scrape_single_race(race_date, venue, race_no)\n",
        "                    \n",
        "                    if success:\n",
        "                        total_processed += 1\n",
        "                        print(f\"    📈 Progress: {total_processed}/{max_races if max_races else '∞'}\")\n",
        "                    \n",
        "                    # Rate limiting\n",
        "                    time.sleep(Config.RATE_LIMIT)\n",
        "                    \n",
        "                    # Save data periodically\n",
        "                    if total_processed % Config.BATCH_SIZE == 0 and total_processed > 0:\n",
        "                        print(f\"\\\\n💾 Saving batch at {total_processed} races...\")\n",
        "                        scraper.save_data(f\"batch_{total_processed//Config.BATCH_SIZE:03d}\")\n",
        "        \n",
        "        # Final save\n",
        "        print(f\"\\\\n💾 Final save...\")\n",
        "        final_file = scraper.save_data(\"final\")\n",
        "        \n",
        "        # Summary\n",
        "        print(f\"\\\\n📊 === SCRAPING SUMMARY ===\")\n",
        "        print(f\"✅ Total races processed: {scraper.processed_count}\")\n",
        "        print(f\"📁 Total data records: {len(scraper.all_data)}\")\n",
        "        print(f\"❌ Errors encountered: {len(scraper.errors)}\")\n",
        "        print(f\"💾 Final data file: {final_file}\")\n",
        "        \n",
        "        return scraper\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"💥 Fatal error: {e}\")\n",
        "        return None\n",
        "        \n",
        "    finally:\n",
        "        # Always clean up\n",
        "        scraper.stop_browser()\n",
        "        print(\"🧹 Cleanup completed\")\n",
        "\n",
        "# Test the utility functions\n",
        "print(\"🔧 Testing utility functions...\")\n",
        "test_dates = generate_race_dates('2024-01-01', '2024-01-31')\n",
        "print(f\"✅ Generated {len(test_dates)} race dates for January 2024\")\n",
        "print(f\"📅 Sample dates: {[d.strftime('%Y-%m-%d %A') for d in test_dates[:3]]}\")\n",
        "print(f\"🏟️ Venues: {get_race_venues()}\")\n",
        "print(f\"🏇 Race numbers: {get_race_numbers()}\")\n",
        "print(\"✅ All utility functions ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# 🧪 TEST RUN - Small sample\n",
        "print(\"🧪 === TEST RUN ===\")\n",
        "print(\"Running test with small sample to verify everything works...\")\n",
        "\n",
        "# Test with just a few races from January 2024\n",
        "test_result = run_hkjc_scraper(\n",
        "    start_date='2024-01-03',  # A Wednesday  \n",
        "    end_date='2024-01-07',    # A Sunday\n",
        "    venues=['ST'],            # Just Sha Tin\n",
        "    max_races=3              # Only 3 races total\n",
        ")\n",
        "\n",
        "print(\"\\\\n🧪 === TEST COMPLETED ===\")\n",
        "if test_result:\n",
        "    print(\"✅ Test successful! The scraper is working correctly.\")\n",
        "    print(\"💡 You can now run larger scraping jobs.\")\n",
        "else:\n",
        "    print(\"❌ Test failed. Check the error messages above.\")\n",
        "    print(\"💡 Make sure Chrome browser is installed and internet connection is stable.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# 🚀 FULL SCRAPER EXAMPLES\n",
        "print(\"🚀 === READY FOR FULL SCRAPING ===\")\n",
        "print(\"Uncomment one of the examples below to run larger scraping jobs:\")\n",
        "\n",
        "\n",
        "print(\"\\\\n📊 Example 3: Large dataset...\")\n",
        "result3 = run_hkjc_scraper(\n",
        "    start_date='2024-05-26', #start from here\n",
        "    end_date='2025-07-14',\n",
        "    venues=['ST', 'HV'], \n",
        "    max_races=None\n",
        ")\n",
        "\n",
        "print(\"\\\\n📋 === USAGE INSTRUCTIONS ===\")\n",
        "print(\"1. ✅ Run the test above first to verify everything works\")\n",
        "print(\"2. 🔧 Modify the date ranges and venues as needed\")\n",
        "print(\"3. 🚀 Uncomment one of the examples above\")\n",
        "print(\"4. ⏰ Be patient - scraping takes time due to rate limiting\")\n",
        "print(\"5. 📁 All data will be saved to CSV files in the output/ directory\")\n",
        "\n",
        "print(\"\\\\n📊 === DATA OUTPUT ===\")\n",
        "print(\"• Main data file: hkjc_data_TIMESTAMP.csv (all data combined)\")\n",
        "print(\"• Race info: hkjc_data_race_info_TIMESTAMP.csv\")\n",
        "print(\"• Performance data: hkjc_data_performance_TIMESTAMP.csv\") \n",
        "print(\"• Error log: hkjc_data_errors_TIMESTAMP.txt (if any errors)\")\n",
        "\n",
        "print(\"\\\\n✨ === FEATURES ===\")\n",
        "print(\"✅ No CORS issues (uses Selenium)\")\n",
        "print(\"✅ Real weather data (HKO API)\")\n",
        "print(\"✅ Race results and horse data\")\n",
        "\n",
        "\n",
        "print(\"✅ Automatic CSV export\")\n",
        "print(\"✅ Error handling and recovery\") \n",
        "print(\"✅ Rate limiting (respectful scraping)\")\n",
        "print(\"✅ Progress tracking\")\n",
        "\n",
        "print(\"\\\\n🎯 Ready to scrape! Uncomment an example above to start.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# 📊 DATA ANALYSIS UTILITIES\n",
        "def analyze_scraped_data(output_dir=\"./output/\"):\n",
        "    \"\"\"Analyze the scraped data files\"\"\"\n",
        "    import glob\n",
        "    \n",
        "    print(\"📊 === DATA ANALYSIS ===\")\n",
        "    \n",
        "    # Find all CSV files\n",
        "    csv_files = glob.glob(f\"{output_dir}*.csv\")\n",
        "    \n",
        "    if not csv_files:\n",
        "        print(\"⚠️ No CSV files found in output directory!\")\n",
        "        print(f\"📁 Looking in: {output_dir}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"📁 Found {len(csv_files)} CSV files:\")\n",
        "    \n",
        "    for file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            filename = file.split('/')[-1]\n",
        "            print(f\"\\\\n📄 {filename}:\")\n",
        "            print(f\"  📊 Rows: {len(df):,}\")\n",
        "            print(f\"  📊 Columns: {len(df.columns)}\")\n",
        "            \n",
        "            # Show data types if it's a performance file\n",
        "            if 'performance' in filename:\n",
        "                print(f\"  🏇 Unique horses: {df['horse_name'].nunique() if 'horse_name' in df.columns else 'N/A'}\")\n",
        "                print(f\"  🏁 Unique races: {len(df.groupby(['date', 'venue', 'race_no'])) if all(col in df.columns for col in ['date', 'venue', 'race_no']) else 'N/A'}\")\n",
        "            \n",
        "            # Show weather data summary\n",
        "            if 'race_info' in filename and 'weather' in df.columns:\n",
        "                print(f\"  🌤️ Weather data available: Yes\")\n",
        "            \n",
        "            # Show column names (first few)\n",
        "            print(f\"  📋 Sample columns: {list(df.columns[:5])}\")\n",
        "            \n",
        "            # Show sample data\n",
        "            if len(df) > 0:\n",
        "                print(f\"  📝 Sample data:\")\n",
        "                sample_cols = df.columns[:4]  # First 4 columns\n",
        "                print(f\"    {df[sample_cols].head(1).to_string(index=False, max_cols=4)}\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error reading {file}: {e}\")\n",
        "    \n",
        "    print(f\"\\\\n✅ Analysis complete!\")\n",
        "\n",
        "def combine_data_files(output_dir=\"./output/\"):\n",
        "    \"\"\"Combine multiple batch files if they exist\"\"\"\n",
        "    import glob\n",
        "    \n",
        "    print(\"🔗 === COMBINING DATA FILES ===\")\n",
        "    \n",
        "    # Look for batch files\n",
        "    performance_files = glob.glob(f\"{output_dir}*performance*.csv\")\n",
        "    race_info_files = glob.glob(f\"{output_dir}*race_info*.csv\")\n",
        "    \n",
        "    if len(performance_files) > 1:\n",
        "        print(f\"🔗 Combining {len(performance_files)} performance files...\")\n",
        "        all_performance = pd.concat([pd.read_csv(f) for f in performance_files], ignore_index=True)\n",
        "        combined_file = f\"{output_dir}combined_performance_data.csv\"\n",
        "        all_performance.to_csv(combined_file, index=False)\n",
        "        print(f\"✅ Combined performance data: {len(all_performance):,} records → {combined_file}\")\n",
        "    \n",
        "    if len(race_info_files) > 1:\n",
        "        print(f\"🔗 Combining {len(race_info_files)} race info files...\")\n",
        "        all_race_info = pd.concat([pd.read_csv(f) for f in race_info_files], ignore_index=True)\n",
        "        combined_file = f\"{output_dir}combined_race_info.csv\"\n",
        "        all_race_info.to_csv(combined_file, index=False)\n",
        "        print(f\"✅ Combined race info: {len(all_race_info):,} records → {combined_file}\")\n",
        "    \n",
        "    print(\"✅ Combining complete!\")\n",
        "\n",
        "# Run analysis on any existing data\n",
        "\n",
        "print(\"🔍 Checking for existing data files...\")\n",
        "analyze_scraped_data()\n",
        "combine_data_files()\n",
        "\n",
        "print(\"\\\\n📋 === ANALYSIS FUNCTIONS READY ===\")\n",
        "print(\"• analyze_scraped_data() - Analyze all CSV files\")  \n",
        "print(\"• combine_data_files() - Combine multiple batch files\")\n",
        "print(\"\\\\n💡 Run these functions after scraping to analyze your data!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
